{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find this kernel helpful, Please check and upvote the original notebook by @cheongwoongkang from which I forked. I just fine tuned it little bit.\n",
    "https://www.kaggle.com/cheongwoongkang/roberta-baseline-starter-simple-postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "pd_train = pd.read_csv('train.csv')\n",
    "pd_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(pd_train)\n",
    "test = np.array(pd_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "I formulate this task as an extractive question answering problem, such as SQuAD.  \n",
    "Given a question and context, the model is trained to find the answer spans in the context.\n",
    "\n",
    "Therefore, I use sentiment as question, text as context, selected_text as answer.\n",
    "- Question: sentiment\n",
    "- Context: text\n",
    "- Answer: selected_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(input_str, search_str):\n",
    "    l1 = []\n",
    "    length = len(input_str)\n",
    "    index = 0\n",
    "    while index < length:\n",
    "        i = input_str.find(search_str, index)\n",
    "        if i == -1:\n",
    "            return l1\n",
    "        l1.append(i)\n",
    "        index = i + 1\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan <class 'float'>\n",
      "nan <class 'float'>\n",
      "neutral <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Convert training data\n",
    "\n",
    "output = {}\n",
    "output['version'] = 'v1.0'\n",
    "output['data'] = []\n",
    "\n",
    "for line in train:\n",
    "    paragraphs = []\n",
    "    \n",
    "    context = line[1]\n",
    "    \n",
    "    qas = []\n",
    "    question = line[-1]\n",
    "    qid = line[0]\n",
    "    answers = []\n",
    "    answer = line[2]\n",
    "    if type(answer) != str or type(context) != str or type(question) != str:\n",
    "        print(context, type(context))\n",
    "        print(answer, type(answer))\n",
    "        print(question, type(question))\n",
    "        continue\n",
    "    answer_starts = find_all(context, answer)\n",
    "    for answer_start in answer_starts:\n",
    "        answers.append({'answer_start': answer_start, 'text': answer})\n",
    "    qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n",
    "    \n",
    "    paragraphs.append({'context': context, 'qas': qas})\n",
    "    output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n",
    "\n",
    "with open('data/train.json', 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data\n",
    "\n",
    "output = {}\n",
    "output['version'] = 'v1.0'\n",
    "output['data'] = []\n",
    "\n",
    "for line in test:\n",
    "    paragraphs = []\n",
    "    \n",
    "    context = line[1]\n",
    "    \n",
    "    qas = []\n",
    "    question = line[-1]\n",
    "    qid = line[0]\n",
    "    if type(context) != str or type(question) != str:\n",
    "        print(context, type(context))\n",
    "        print(answer, type(answer))\n",
    "        print(question, type(question))\n",
    "        continue\n",
    "    answers = []\n",
    "    answers.append({'answer_start': 1000000, 'text': '__None__'})\n",
    "    qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n",
    "    \n",
    "    paragraphs.append({'context': context, 'qas': qas})\n",
    "    output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n",
    "\n",
    "with open('data/test.json', 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the pytorch-transformers package (v2.5.1) of [huggingface](https://github.com/huggingface/transformers/tree/v2.5.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir results_roberta_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune a RoBERTa-QA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"transformers/examples/run_squad.py\", line 27, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/run_squad.py \\\n",
    "--model_type roberta \\\n",
    "--model_name_or_path roberta-large \\\n",
    "--do_lower_case \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--data_dir ./data \\\n",
    "--cache_dir /kaggle/input/cached-roberta-large-pretrained/cache \\\n",
    "--train_file train.json \\\n",
    "--predict_file test.json \\\n",
    "--learning_rate 3e-5 \\\n",
    "--weight_decay 1e-2 \\\n",
    "--num_train_epochs 3 \\\n",
    "--max_seq_length 192 \\\n",
    "--doc_stride 64 \\\n",
    "--output_dir results_roberta_large \\\n",
    "--per_gpu_eval_batch_size=16 \\\n",
    "--per_gpu_train_batch_size=16 \\\n",
    "--save_steps=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy predictions to submission file.\n",
    "predictions = json.load(open('results_roberta_large/predictions_.json', 'r'))\n",
    "submission = pd.read_csv(open('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv', 'r'))\n",
    "for i in range(len(submission)):\n",
    "    id_ = submission['textID'][i]\n",
    "    if pd_test['sentiment'][i] == 'neutral': # neutral postprocessing\n",
    "        submission.loc[i, 'selected_text'] = pd_test['text'][i]\n",
    "    else:\n",
    "        submission.loc[i, 'selected_text'] = predictions[id_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission file.\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantic_kaggle",
   "language": "python",
   "name": "quantic_kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
